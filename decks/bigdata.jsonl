{"front": "What are the '5 Vs' that characterize Big Data?", "back": "1. Value: Extracting knowledge from data is valuable\n2. Volume: Very large amounts of data (TB/PB scale)\n3. Variety: Different data formats (structured, semi-structured, unstructured)\n4. Velocity: High speed of data generation\n5. Veracity: Reliability of data used for decisions"}
{"front": "What is the key drawback of the Parameter Server architecture for distributed gradient aggregation?", "back": "The Parameter Server becomes a communication bottleneck since all workers must communicate with it.\n With K parameter servers and p workers, the communication cost is $\\max(n/\\beta, (pn)/(K \\beta))$\n where n is the gradient vector size and $\\beta$ is network bandwidth."}
{"front": "What are the three main forms of parallelism in distributed deep learning?", "back": "1. Data Parallelism: Each worker has full model copy, processes different data batches\n2. Pipeline Parallelism: Model split into stages across devices, process different mini-batches\n3. Operator/Tensor Parallelism: Individual operations split across devices"}
{"front": "What is Ring AllReduce and what are its key characteristics?", "back": "Ring AllReduce is a decentralized gradient aggregation algorithm where workers form a logical ring. It has:\n- Communication volume: 2n (n = gradient size)\n- Number of steps: 2(p-1) (p = number of workers)\n- Two phases: Reduce-scatter and All-gather\n- Each worker only communicates with neighbors"}
{"front": "What distinguishes TPUs from GPUs architecturally?", "back": "TPUs use a systolic array architecture for matrix multiplication rather than the SIMT architecture of GPUs. Key differences:\n- Non von Neumann architecture (dataflow)\n- Specialized for tensor operations\n- Uses bfloat16 precision\n- Has Matrix Multiplication Unit (MXU)\n- Less flexible but more efficient for deep learning"}
{"front": "What are the three main approaches to overcome TCP/IP networking limitations?", "back": "1. Move functionalities to NIC hardware (DMA, interrupt coalescing, TCP offloading)\n2. Exploit multicore CPUs to process packets in parallel\n3. Bypass OS and implement processing in user-space"}
{"front": "What is the 'bubble overhead' problem in pipeline parallelism and how is it addressed?", "back": "Bubble overhead refers to GPU idle time in naive pipeline parallelism due to sequential dependencies. It's addressed by:\n1. Dividing mini-batches into M micro-batches\n2. Pipelining Forward/Backward computations across micro-batches\n3. Results in smaller bubbles and better GPU utilization"}
{"front": "What are the key differences between blocking and non-blocking fat trees?", "back": "Non-blocking fat trees:\n- Full bisection bandwidth\n- Higher cost (more switches/links)\n- Equal bandwidth at all levels\n\nBlocking fat trees:\n- Reduced bandwidth at higher levels\n- Lower cost\n- Used in practice (e.g., Meta uses 7:1 blocking)"}
{"front": "What are the two main issues that need to be solved in PageRank?", "back": "1. Dead End Problem:\n- Pages with no outlinks cause PageRank to leak out\n- Makes matrix non-stochastic\n\n2. Spider Trap Problem:\n- Not every node is reachable\n- PageRank gets absorbed by few pages\n- Makes convergence impossible"}
{"front": "What are the tradeoffs between RDMA one-sided operations and two-sided operations?", "back": "One-sided operations:\n- Lower latency (no remote CPU involvement)\n- Better performance (4.6x-10x faster)\n- Limited functionality\n\nTwo-sided operations:\n- Higher latency (remote CPU involved)\n- More flexible\n- Support for complex operations"}
{"front": "What is gradient compression and what are its main types?", "back": "Three main types:\n1. Quantization: Reduce bitwidth (e.g., float32 â†’ float16)\n2. Sparsification: Sample subset of elements (e.g., top-k by magnitude)\n3. Low rank: Decompose gradient into low-rank matrices ($M \\approx PQ^T$)\n\nTrade-off: Faster communication but may need more iterations to converge"}
{"front": "What are the two competing recursive doubling algorithms for gradient aggregation?", "back": "1. Bandwidth-Optimal Recursive Doubling:\n- Communication volume: 2n\n- Steps: 2log_2(p)\n- Better for large vectors\n\n2. Latency-Optimal Recursive Doubling:\n- Communication volume: $n\\log_2(p)$\n- Steps: $\\log_2(p)$\n- Better for small vectors"}
{"front": "What problems does RDMA solve compared to traditional TCP/IP networking?", "back": "1. Eliminates OS involvement in data path\n2. Enables zero-copy data transfer\n3. Reduces CPU utilization\n4. Provides lower latency ($\\approx 1 \\mu s$ vs $\\approx 10 \\mu s$)\n5. Separates control path (slow) from data path (fast)\n6. Allows direct memory access without remote CPU involvement"}
{"front": "How does the Dragonfly network topology achieve low diameter?", "back": "Dragonfly achieves diameter=3 through:\n1. Fully connected groups of switches\n2. Groups connected to each other with at least one link\n3. Traffic can reach any destination in 3 hops:\n   - Up to group leader\n   - To destination group\n   - Down to destination node"}
{"front": "What distinguishes a SmartNIC from a regular NIC?", "back": "SmartNIC features:\n1. Has processing units (cores, FPGA, etc.)\n2. Can process packets before delivery\n3. Enables computation-communication overlap\n4. Reduces CPU utilization\n5. Supports customizable packet processing\n6. Can offload network protocols"}
{"front": "What are the three key problems faced by recommender systems?", "back": "1. Cold Start: Insufficient data for new users/items\n2. Scalability: System requires significant computational power with millions of users/items\n3. Sparsity: Most items not rated by users, leading to sparse user-item matrix"}
{"front": "How does the curse of dimensionality affect similarity measures in high dimensions?", "back": "Effects:\n1. Points become equidistant\n2. Distance between nearest/farthest points becomes indistinguishable\n3. Most points lie near the edges of the space\n4. Probability of finding points not at edge decreases exponentially with dimensions\n5. Makes clustering difficult"}
{"front": "What are the advantages and disadvantages of the Jellyfish network topology?", "back": "Advantages:\n- Better performance than fat trees with same resources\n- Flexible incremental expansion\n- Random topology adapts to traffic patterns\n\nDisadvantages:\n- Complex configuration/management\n- Irregular structure makes job allocation difficult\n- Conservative datacenter operators reluctant to adopt"}
{"front": "What are the key components of Google's TPU architecture?", "back": "1. Matrix Multiplication Unit (MXU):\n- Systolic array architecture\n- Optimized for tensor operations\n- Uses bfloat16 precision\n\n2. Unified Buffer:\n- High-bandwidth SRAM\n- Feeds data to MXU\n\n3. Activation Unit:\n- Applies non-linear functions\n\n4. Control Unit:\n- Manages dataflow"}
{"front": "How does asynchronous distributed SGD differ from synchronous, and what are its challenges?", "back": "Differences:\n- No barrier synchronization\n- Workers continue without waiting\n- Gradients aggregated every S iterations\n- Can ignore stragglers\n\nChallenges:\n- Stale parameters degrade convergence\n- Theoretical guarantees harder\n- Trade-off between staleness and throughput"}
{"front": "What factors determine whether Bandwidth-Optimal or Latency-Optimal Recursive Doubling is better for allreduce?", "back": "Cost for Bandwidth-Optimal: $2\\log_2(p)\\alpha + \\frac{2n}{\\beta}$\nCost for Latency-Optimal: $\\log_2(p)\\alpha + \\frac{(\\log_2(p)n)}{\\beta}$\n\nLatency-Optimal wins when: $\\log_2(p)\\alpha + \\frac{(\\log_2(p)n)}{\\beta} < 2\\log_2(p)\\alpha + \\frac{2n}{\\beta}$\n\nSimplifying: Better for small n (vector size) and large $\\alpha$ (step overhead)"}
{"front": "How does the Ring AllReduce algorithm achieve optimal bandwidth utilization compared to Parameter Server?", "back": "Ring AllReduce optimality:\n1. Each link carries exactly n/p elements per step\n2. Takes 2(p-1) steps total\n3. Total volume = 2n, independent of p\n4. Each node's bandwidth fully utilized\n\nParameter Server bottleneck:\nServer receives pn/K total data where K is number of servers, creating bandwidth bottleneck of O(pn)"}
{"front": "Explain how deadlocks can occur in a Dragonfly topology and how they are prevented", "back": "Deadlocks occur when:\n1. Packets form a cyclic dependency\n2. No buffer space available in cycle\n\nPrevention methods:\n1. Virtual Channels (VCs)\n2. Dateline scheme: assign ordered channels\n3. Escape paths: guarantee deadlock-free path\n4. Bubble flow control\n\nCost: Extra buffer space required for VCs"}
{"front": "What makes pipeline parallelism particularly challenging with variable-length inputs like text?", "back": "Challenges:\n1. Uneven computation time per stage\n2. Dynamic batch sizes needed\n3. Load imbalance between GPUs\n4. Bubble size varies with input length\n5. Complex scheduling required\n6. Memory footprint varies\n7. Hard to optimize pipeline depth\n8. Communication patterns become irregular"}
{"front": "How does non-uniform memory access between GPUs within a server affect distributed training?", "back": "Impact of NUMA effects:\n1. Different bandwidth between GPU pairs\n   - Direct NVLink: ~300GB/s\n   - Through CPU: ~64GB/s\n   - PCIe switches: ~128GB/s\n2. Requires topology-aware placement\n3. Affects algorithm selection\n   - Ring vs Tree algorithms\n   - Pipeline stage mapping\n4. Can cause stragglers if ignored"}
{"front": "Compare the failure modes of blocking vs non-blocking fat trees under link failures", "back": "Non-blocking fat trees:\n- Graceful degradation\n- Multiple alternate paths\n- Performance proportional to failed links\n\nBlocking fat trees:\n- Catastrophic failures possible\n- Limited alternate paths\n- Single link can disconnect subtree\n- Recovery requires global reconfiguration\n\nMakes blocking trees riskier for critical workloads"}
{"front": "Explain how the computational complexity of PageRank implementation changes when dealing with dangling nodes", "back": "With dangling nodes:\n1. Matrix M' is sparse but not stochastic\n2. Need teleporting factor (1-d)/N\n3. Complexity per iteration:\n   - Sparse matrix-vector: O(E)\n   - Dense vector addition: O(N)\n4. Memory needed:\n   - Sparse matrix: $O(E)$\n   - Two vectors: $O(N)$\nWhere E=edges, N=nodes"}
{"front": "What makes dynamic load balancing particularly challenging in a Dragonfly+ topology?", "back": "Challenges:\n1. Non-uniform path lengths\n2. Local vs global routing decisions\n3. Multiple valid paths per flow\n4. Congestion feedback delayed\n5. Path diversity varies by source-destination\n6. Local decisions can cause global congestion\n7. Trade-off between minimal/non-minimal paths\n8. Group-level vs switch-level balancing"}
{"front": "Compare the memory access patterns and efficiency between a systolic array (TPU) and SIMT (GPU) for matrix multiplication", "back": "Systolic Array (TPU):\n- Predictable access pattern\n- Data reuse through neighbors\n- No cache needed\n- Higher arithmetic intensity\n- Fixed dataflow\n\nSIMT (GPU):\n- Cache-dependent performance\n- Complex memory hierarchy\n- Thread block synchronization\n- Flexible but less efficient\n- Higher memory bandwidth needed"}
{"front": "What are the key challenges in load balancing on a DragonFly+ network with minimal/non-minimal paths?", "back": "1. Path length diversity: minimal vs non-minimal paths have different costs\n2. Congestion prediction: hard to estimate future congestion\n3. Local vs global optimization: local decisions can cause global hotspots\n4. Adaptivity timing: when to switch between minimal/non-minimal paths\n5. Group-level fairness: balancing between different groups\n6. Asymmetric bandwidth: different capacities between local/global links"}
{"front": "What makes content-based and collaborative filtering hybrid systems more effective than pure approaches?", "back": "Hybrid advantages:\n1. Overcomes cold start by using content when no ratings exist\n2. Reduces sparsity through content-based predictions\n3. Improves accuracy by combining signals\n4. Enables cross-domain recommendations\n5. More robust to different user/item types\n6. Can adapt ratio of content/collaborative based on data availability"}
{"front": "How does the curse of dimensionality specifically affect cosine similarity in high dimensions?", "back": "Effects on cosine similarity:\n1. Vectors become nearly orthogonal\n2. Most angles cluster around 90 degrees\n3. Similarity scores concentrate around 0\n4. Relative differences become less meaningful\n5. Random vectors appear equally dissimilar\n6. Need for dimensionality reduction before similarity computation"}
{"front": "What are the tradeoffs between different load balancing granularities (packet vs flowlet vs flow)?", "back": "Packet level:\n+ Perfect load balance\n- Reordering issues\n- High overhead\n\nFlowlet level:\n+ No reordering if $gap > \\delta_{delay}$\n- Requires careful gap parameter tuning\n- Depends on traffic burstiness\n\nFlow level:\n+ No reordering\n+ Lower overhead\n- Potential collisions between elephant flows"}
{"front": "How do matrix factorization recommender systems handle temporal dynamics?", "back": "Temporal handling techniques:\n1. Time-aware factors: $r_{ui}(t) = \\mu + b_u(t) + b_i(t) + p_u(t)^Tq_i(t)$\n2. Time decay for older ratings\n3. Session-based factorization\n4. Drift detection in user preferences\n5. Seasonal patterns modeling\n6. Evolving feature vectors"}
{"front": "Compare the effectiveness of different similarity measures for sparse high-dimensional data", "back": "Measure comparisons:\nCosine:\n+ Scale invariant\n- Sensitive to zero entries\n\nJaccard:\n+ Handles sparse data well\n- Loses magnitude information\n\nPearson:\n+ Handles different rating scales\n- Requires sufficient overlap\n\nAdjusted cosine:\n+ Accounts for rating bias\n- Computationally expensive"}
{"front": "What are the key considerations in designing an adaptive load balancing system for heterogeneous GPU clusters?", "back": "Design considerations:\n1. Different GPU compute capabilities\n2. Variable NVLink/PCIe connectivity\n3. Dynamic workload characteristics\n4. Communication topology awareness\n5. Power/thermal constraints\n6. Job placement impact\n7. Memory hierarchy differences\n8. Fault tolerance requirements"}
{"front": "How does the choice of similarity measure affect the cold-start problem in recommender systems?", "back": "Impact by measure:\n\nContent-based similarity:\n+ Works with no ratings\n- Requires good feature engineering\n\nCollaborative similarity:\n- Fails completely for new items\n+ Better for established items\n\nHybrid similarity:\n+ Graceful degradation\n+ Can weight by confidence"}
{"front": "What techniques can be used to detect and mitigate load imbalance in distributed recommendation serving?", "back": "Detection & mitigation:\n1. Request rate monitoring per shard\n2. Latency percentile tracking\n3. Dynamic shard rebalancing\n4. Replica count adjustment\n5. Smart request routing\n6. Caching popular items\n7. Load-aware consistent hashing\n8. Backup request scheduling"}
{"front": "How do different similarity measures perform when dealing with adversarial perturbations?", "back": "Robustness comparison:\n\nEuclidean:\n- Very sensitive to perturbations\n- No bounds on impact\n\nCosine:\n+ Bounded impact on angle\n- Sensitive to small values\n\nRank-based:\n+ More robust to noise\n- Loses magnitude information"}
{"front": "Compare Parameter Server vs Ring AllReduce load balancing approaches", "back": "Parameter Server:\n- Centralized bottleneck at server\n- Communication cost: max(n, pn/K)\n- Single point of failure\n- Simple implementation\n\nRing AllReduce:\n- Decentralized\n- Communication cost: 2n\n- Better bandwidth utilization\n- More complex implementation\n- Each node only communicates with neighbors"}
{"front": "What is ECMP (Equal-Cost Multi-Path) load balancing and its limitations?", "back": "ECMP:\n- Randomly allocates flows to equal-cost paths using flow hash\n- Agnostic to available resources\n- Main limitations:\n  1. Long-lasting collisions between elephant flows\n  2. No congestion awareness\n  3. Cannot handle link failures gracefully\n  4. Poor performance with skewed flow sizes"}
{"front": "How does CONGA's flowlet-based load balancing work?", "back": "CONGA:\n1. Tracks congestion on all paths between leaf switches\n2. Measures congestion as maximum queue length on path\n3. Sends flowlets on least congested path\n4. Uses flowlet switching to avoid reordering\n5. Requires switch support for congestion tracking\n6. Challenges with flowlet gap parameter tuning"}
{"front": "Compare centralized vs distributed load balancing approaches", "back": "Centralized:\n+ Global optimization possible\n+ Simpler consistency\n- Single point of failure\n- Scalability bottleneck\n- High latency decisions\n\nDistributed:\n+ Better scalability\n+ Lower latency\n+ More fault tolerant\n- Complex consistency\n- Local decisions can be suboptimal\n- Higher overhead"}
{"front": "What is Hedera and how does it handle elephant flows?", "back": "Hedera:\n1. Polls edge switches for flow statistics\n2. Detects flows exceeding threshold (e.g., >10% link capacity)\n3. Estimates flow demands\n4. Centrally computes optimal placement\n5. Limitations:\n   - Reactive to large flows\n   - High monitoring overhead\n   - Slow reaction to changes"}
{"front": "How does Flowbender implement congestion-aware load balancing?", "back": "Flowbender:\n1. Host-based approach requiring no switch changes\n2. Uses ECN marks as congestion signal\n3. Forces ECMP rehashing when congestion detected\n4. Flow-level granularity to avoid reordering\n5. Advantages:\n   - Simple implementation\n   - No specialized hardware\n   - Works with existing switches"}
{"front": "What are the key differences between packet spraying and flowlet switching?", "back": "Packet Spraying:\n- Per-packet granularity\n- Perfect load balance\n- High reordering probability\n- No state needed\n\nFlowlet Switching:\n- Burst-level granularity\n- Good load balance\n- Limited reordering if gap tuned correctly\n- Needs flow state\n- Depends on traffic burstiness"}
{"front": "How do adaptive load balancing algorithms handle path asymmetry in data centers?", "back": "Handling path asymmetry:\n1. Path length aware routing\n2. Bandwidth asymmetry detection\n3. Latency-based path selection\n4. Dynamic congestion measurement\n5. Different weights for local/global paths\n6. Topology-specific optimizations\n7. Separate policies for up/down paths"}
{"front": "Compare reactive vs proactive load balancing strategies", "back": "Reactive:\n+ No prediction needed\n+ Simpler implementation\n- Late response to problems\n- Can oscillate\n\nProactive:\n+ Prevents congestion\n+ Better stability\n- Needs accurate prediction\n- Higher overhead\n- Complex modeling required"}
{"front": "What are the challenges in load balancing RDMA traffic?", "back": "RDMA challenges:\n1. No TCP congestion control\n2. Limited switch buffer space\n3. PFC deadlock risks\n4. Path-dependent performance\n5. Connection-oriented nature\n6. Lack of flow information\n7. Hardware offload constraints\n8. Priority handling complexity"}
{"front": "What is a sparsification technique for gradient compression in distributed deep learning?", "back": "Top-k sparsification selects only the k largest magnitude elements from the gradient vector, reducing communication overhead while maintaining convergence guarantees."}
{"front": "How does SRD differ from RoCEv2?", "back": "SRD (Amazon's protocol) allows out-of-order packet delivery and uses fewer Queue Pairs for better scalability, while RoCEv2 maintains strict ordering and has more complex congestion control."}
{"front": "What is the bisection bandwidth of a 2D torus topology with n nodes?", "back": "$2\\sqrt{n}$ links"}
{"front": "What is flowlet switching in CONGA?", "back": "A technique where bursts of packets from same flow separated by time interval are routed together, avoiding reordering while allowing path changes between flowlets for load balancing"}
{"front": "What is the key difference between on-path and off-path SmartNICs?", "back": "On-path SmartNICs process all packets directly in the datapath, while off-path SmartNICs can selectively route packets either through processing cores or directly to egress"}
{"front": "In a fat tree topology, how does the number of links change at each level?", "back": "The number of links doubles at each level going up the tree, enabling non-blocking communication but requiring switches with different port counts"}
{"front": "What is the communication volume and number of steps for Ring AllReduce?", "back": "Volume: $2n$ (n = gradient size), Steps: $2(p-1)$ (p = number of workers)"}
{"front": "How does fixed point arithmetic help with in-network aggregation?", "back": "Switches lack FP units, so fixed point allows numeric operations without dedicated hardware. Values are scaled to integers, operations performed, then rescaled back"}
{"front": "What is the main drawback of asynchronous distributed SGD?", "back": "Stale parameters degrade training convergence since theory and empirical evidence support using fresher parameter values"}
{"front": "In Dragonfly topology, what causes deadlocks and how are they prevented?", "back": "Loops in the topology can cause circular dependencies between packets. Prevention requires extra hardware resources to implement virtual channels"}
{"front": "What is the difference between a folded CLOS network and a fat tree?", "back": "A folded CLOS uses switches of uniform radix by 'slicing' larger switches into multiple smaller ones, while maintaining the same bisection bandwidth as a fat tree"}
{"front": "Why might Token-bucket congestion control be insufficient for RDMA?", "back": "Token-bucket operates too slowly for RDMA's microsecond-scale latencies and doesn't account for complex congestion patterns in modern datacenter networks"}
{"front": "In what scenario would Lat. Optimal Recursive Doubling outperform Bw. Optimal?", "back": "When gradient vector size n is small, since Lat. Optimal has cost \n$\\log_2(p)\\alpha + (\\log_2(p)n)/\\beta$ \nvs\n Bw. Optimal's $2\\log_2(p)\\alpha + 2n/\\beta$"}
{"front": "What problem does probabilistic aggregation solve in in-network reduction?", "back": "It maintains high performance under congestion by allowing switches to skip aggregating some packets, trading accuracy for robustness"}
{"front": "Why is DragonFly+ easier to expand than regular DragonFly?", "back": "DragonFly+ uses fat trees within groups instead of full mesh connectivity, making it easier to add new nodes without reconfiguring existing connections"}
{"front": "What is the key advantage of Hamming meshes for deep learning workloads?", "back": "Combines cheap toroidal network with fat trees at edges, creating 'information highways' for edge-to-edge communication in logarithmic hops while maintaining cost efficiency"}
{"front": "How does Random Packet Spraying differ from ECMP?", "back": "Random Packet Spraying selects random paths per-packet while ECMP hashes flow identifiers for path selection, trading packet ordering for better fault tolerance"}
{"front": "What advantage does user-space networking have over kernel networking?", "back": "Avoids system call overhead and kernel context switches, allowing direct NIC access from user space for reduced latency"}
{"front": "What property makes TPUs more efficient than GPUs for matrix operations?", "back": "TPUs use systolic array architecture, allowing data to flow through ALUs without memory access, reducing Von Neumann bottleneck"}
{"front": "What is the time complexity of computing a gradient reduction on K parameter servers?", "back": "$\\max(\\frac{n}{\\beta}, \\frac{pn}{K\\beta})$ where n is gradient size, p is workers, $\\beta$ is bandwidth"}
{"front": "In RDMA's RC mode, what does one-sided mean?", "back": "Operations can be initiated by one peer without involvement of the remote CPU/OS, as memory regions are pre-registered and known"}
{"front": "How does pipeline parallelism reduce bubble overhead?", "back": "Divides mini-batch into M micro-batches to overlap forward/backward computations across layers, reducing idle time between phases"}
{"front": "What is the relationship between data parallelism batch size and scalability?", "back": "Scalability is limited by mini-batch size as too large batches harm convergence, despite perfect parallelization potential"}
{"front": "What makes SoftRoCE useful for incremental RDMA deployment?", "back": "Allows systems with non-RDMA NICs to communicate with RDMA-enabled systems through software emulation, enabling gradual hardware upgrades"}
{"front": "Why might increased switch radix not improve a Dragonfly network?", "back": "Higher radix increases full-mesh connectivity requirements within groups, potentially making the network more expensive without proportional performance gain"}
{"front": "What problem arises when using multiple parameter servers for gradient aggregation?", "back": "Parameter sharding must be balanced across servers to avoid bottlenecks, but optimal sharding is workload-dependent and hard to determine"}
{"front": "How does operator parallelism differ from pipeline parallelism in communication pattern?", "back": "Operator parallelism requires frequent communication during forward/backward passes, while pipeline parallelism only communicates at layer boundaries"}
{"front": "What makes in-network aggregation challenging for IEEE 754 floating point values?", "back": "Non-associativity of floating point operations means results depend on aggregation order, which can't be guaranteed in network switches"}
{"front": "Why might a blocking fat tree be preferred over a dragonfly network?", "back": "Simpler topology makes routing and load balancing more predictable, despite potentially higher cost and lower theoretical performance"}
{"front": "What is the diameter of a 2D mesh with n nodes?", "back": "$2(\\sqrt{n} - 1)$ hops"}
{"front": "In throughput vs latency oriented design, which cache configuration do GPUs favor?", "back": "Smaller caches with simpler control logic, shared across multiple cores to optimize for throughput over single-thread performance"}
{"front": "What is the cost formula for bandwidth-optimal recursive doubling?", "back": "$2\\log_2(p)\\alpha + 2n/\\beta$ where $\\alpha$ is step cost, $\\beta$ is bandwidth, n is message size, p is processors"}
{"front": "What transport modes does Infiniband support and which allows READ operations?", "back": "RC (Reliable Connected), UC (Unreliable Connected), UD (Unreliable Datagram). Only RC supports READ operations"}
{"front": "What problem arises from sk_buff overhead in Linux networking?", "back": "sk_buff structure is $\\approx 200$ bytes, creating 80% overhead for 64-byte packets and consuming 63% of CPU usage in packet processing"}
{"front": "For ML training, what is the typical ratio of communication to computation time on a 10Gbps network?", "back": "Up to 50% of total training time is spent on gradient communication"}
{"front": "What are the three evolutionary phases of hardware performance scaling?", "back": "1) Moore's Law full effect (pre-2005) 2) Frequency scaling slowdown/manycore era (2005-2015) 3) Specialization through ASICs/GPUs/TPUs (2015-present)"}
{"front": "At what packet rate does 100 Gbps Ethernet operate with 1500B packets?", "back": "8.3 nanoseconds per packet ($\\approx 120$ million packets per second)"}
{"front": "In deep learning training with N GPUs, how many QPs (Queue Pairs) does RoCE require compared to SRD?", "back": "RoCE requires $O(N^2)$ QPs for all-to-all communication while SRD needs only $O(N)$"}
{"front": "What is the memory footprint ratio between sparse and dense sk_buff representation?", "back": "Around 4:1 wastage (232 bytes structure for potentially small packets)"}
{"front": "How much does AWS's SRD protocol save in QP state compared to RoCE for large deployments?", "back": "For N nodes, SRD uses $O(N)$ QPs while RoCE requires $O(N^2)$, critical for large-scale deployments"}
{"front": "What is the typical GPU generation performance increase vs network bandwidth increase ratio?", "back": "GPU compute increases $\\approx 2x$ per generation while network bandwidth increases $\\approx 1.5 x$, widening the communication gap"}
{"front": "When using gradient compression in distributed training, what inequality must hold for speedup?", "back": "$T_{comp} \\cdot I_{comp} < T_{orig} \\cdot I_{orig}$ where T is iteration time and I is number of iterations"}
